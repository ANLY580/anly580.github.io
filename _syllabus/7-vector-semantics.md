---
week: 7
day: Oct 22
title: Vector Semantics
tags: [classification]
---

## Access the Notebooks

- [Notebook](https://mybinder.org/v2/gh/anyl580/lectures/master?urlpath=notebooks/7-vector-semantics/Semantic-vectorization.ipynb.ipynb)

## Readings

- [J&M Chapter 7: Vector Semantics
](https://web.stanford.edu/~jurafsky/slp3/6.pdf)

# Additional Readings

### Fundamentals
- [NLP fundamentals: Maximum Entropy](https://nadesnotes.wordpress.com/2016/09/05/natural-language-processing-nlp-fundamentals-maximum-entropy-maxent/) - simple intro with examples
- [Text Classification with NLTK nand Scikit-Learn](https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html) - Excellent examples of how to operationalize these concepts.
- [Hyper-parameter tuning on datacamp.com](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models)
- Cross-validation - https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f

### Weighted log odds
 - Monroe, Colaresi, and Quinn 2008 - [Fightinâ€™ words: Lexical feature selection and evaluation for identifying the content of political conflict](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863)
 - Jurafsky, Chahuneau, Rutledge, and Smith 2014 - [Narrative framing of consumer sentiment in online restaurant reviews](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863)
 Tyler Schnoebelen's blog - [I dare say you will never use tf-idf again](https://medium.com/@TSchnoebelen/i-dare-say-you-will-never-use-tf-idf-again-4918408b2310)
 
 ### Word2Vec:
  - Ruder's blog - [On word embeddings - Part 3: The secret ingredient](http://ruder.io/secret-word2vec/)
  - Mikolov et al. - [Distributed representations of words nad phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and)
  - Turney and Pantel - [From Frequency to Meaning: Vector space models of semantics](http://www.aaai.org/Papers/JAIR/Vol37/JAIR-3705.pdf)
  - Goldberg and Levy - [Word2vec explained - Deriving Mikolov et al's Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)

## Videos

- Chris Potts [Overview of distributed word representations (10 minutes)](https://www.youtube.com/watch?v=gtuhPq0Xyno&index=1&list=PLfmUaIBTH8exY7fZnJss508Bp8k1R8ASG)
- Chris Potts, [Vector comparison for distributed word representations (10 minutes)](https://www.youtube.com/watch?v=LYH93YnhuyQ&t=304s&index=2&list=PLfmUaIBTH8exY7fZnJss508Bp8k1R8ASG)
- Chris Potts, [Matrix reweighting for distributed word representations (16 minutes)](https://www.youtube.com/watch?v=WFySbJ3FGcM&index=3&list=PLfmUaIBTH8exY7fZnJss508Bp8k1R8ASG)

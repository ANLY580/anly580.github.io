---
week: 7
day: Oct 22
title: Vector Semantics
tags: [classification]
---

## Access the Notebooks

- [Notebook](https://mybinder.org/v2/gh/anyl580/lectures/master?urlpath=notebooks/7-vector-semantics/Semantic-vectorization.ipynb.ipynb)

## Readings

- [J&M Chapter 7: Vector Semantics
](https://web.stanford.edu/~jurafsky/slp3/6.pdf)

# Additional Readings

- [NLP fundamentals: Maximum Entropy](https://nadesnotes.wordpress.com/2016/09/05/natural-language-processing-nlp-fundamentals-maximum-entropy-maxent/) - simple intro with examples
- [Text Classification with NLTK nand Scikit-Learn](https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html) - Excellent examples of how to operationalize these concepts.
- [Hyper-parameter tuning on datacamp.com](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models)
- Cross-validation - https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f

- Weighted log odds:
 - [Fightinâ€™ words: Lexical feature selection and evaluation for identifying the content of political conflict. B. Monroe, M. Colaresi, and K. Quinn, 2008.](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863)
 - [Narrative framing of consumer sentiment in online restaurant reviews by Dan Jurafsky, Victor Chahuneau, Bryan R. Routledge, and Noah A. Smith, 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863)
 - [I dare say you will never use tf-idf again by Tyler Schnoebelen](https://medium.com/@TSchnoebelen/i-dare-say-you-will-never-use-tf-idf-again-4918408b2310)
 - Word2Vec
  - [On word embeddings - Part 3: The secret ingredient](http://ruder.io/secret-word2vec/)


